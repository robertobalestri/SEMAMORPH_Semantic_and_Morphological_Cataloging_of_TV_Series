"""
Simplified recap generation module.

This module provides a streamlined approach to generating "Previously On" recaps
following the core LLM workflow without unnecessary complexity.
"""

from .recap_generator import RecapGenerator
from .models import RecapResult, Event, VideoClip

__all__ = ['RecapGenerator', 'RecapResult', 'Event', 'VideoClip']
"""
LLM services for recap generation.

This module contains all three LLM calls in the recap generation workflow:
1. Query generation per narrative arc
2. Event ranking per arc  
3. Subtitle pruning per event
"""

import json
import logging
import re
import sys
import os
from typing import List, Dict, Any

# Add the src directory to Python path to enable proper imports
current_dir = os.path.dirname(__file__)
src_dir = os.path.dirname(os.path.dirname(current_dir))
if src_dir not in sys.path:
    sys.path.insert(0, src_dir)

from ai_models.ai_models import get_llm, LLMType

logger = logging.getLogger(__name__)


def clean_llm_json_response(response: str) -> Dict:
    """Local implementation to avoid import issues."""
    try:
        # Remove from the string ```json, ```plaintext, ```markdown and ```
        response_cleaned = re.sub(r"```(json|plaintext|markdown)?", "", response)
        response_cleaned = response_cleaned.strip()
        
        # Remove any comments
        response_cleaned = re.sub(r'//.*?$|/\*.*?\*/', '', response_cleaned, flags=re.MULTILINE | re.DOTALL)
        
        # Try to extract a JSON object or array from the cleaned response
        json_match = re.search(r'(\{|\[)[\s\S]*(\}|\])', response_cleaned)
        
        if json_match:
            json_str = json_match.group(0)
            
            # Replace all curly apostrophes with regular apostrophes
            json_str = json_str.replace("'", "'")
            
            try:
                parsed_json = json.loads(json_str)
                
                # If the parsed JSON is a list, return first item or empty dict
                if isinstance(parsed_json, list):
                    return parsed_json[0] if parsed_json else {}
                elif isinstance(parsed_json, dict):
                    return parsed_json
            except json.JSONDecodeError as e:
                logger.error(f"Failed to parse JSON: {e}")
                logger.debug(f"JSON string that failed: {json_str}...")
        
        # If no JSON found, try to find just numbers (fallback)
        numbers = re.findall(r'\b\d+\b', response_cleaned)
        if numbers:
            logger.warning(f"No JSON found, extracted numbers: {numbers}")
            return {"selected_events": [int(x) for x in numbers]}
        
    except Exception as e:
        logger.error(f"Error in clean_llm_json_response: {e}")
    
    # Return empty dict if parsing fails
    logger.warning("Could not parse LLM response, returning empty dict")
    return {}


def generate_arc_queries(season_summary: str, episode_plot: str, narrative_arcs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    LLM #1: Generate vector database queries for each narrative arc.
    
    Args:
        season_summary: Season summary for context
        episode_plot: Current episode plot
        narrative_arcs: List of narrative arcs with title/description
        
    Returns:
        List of query dictionaries for vector database search
    """
    llm = get_llm(LLMType.INTELLIGENT)
    arc_queries = []
    
    for arc in narrative_arcs:
        prompt = f"""Generate 2-3 specific vector database search queries to find historical events that provide context for this narrative arc in the current episode.

**Season Context:** {season_summary}

**Current Episode Plot:** {episode_plot}

**Focus Narrative Arc:**
- Title: {arc['title']}
- Description: {arc['description']}

Generate queries that will find events showing:
1. Development and evolution of this arc
2. Character motivations within this arc  
3. Past conflicts/relationships that impact this arc

The most important thing is that the events proposed should be the most interesting and relevant to the current episode! We are creating a recap to help viewers understand the current episode.
Do not generate phrases such as "search for", "the scene where", "the moment when", "Key interactions where". These lines are all introductory but don't help the research. The query should be a direct narrative moment such as "John confronts Sarah about the missing files".

Output as JSON:
```json
{{
    "queries": [
        {{
            "query_text": "natural language search query",
            "purpose": "what context this provides"
        }}
    ]
}}
```"""

        try:
            response = llm.invoke(prompt)
            query_data = clean_llm_json_response(response.content)
            
            if isinstance(query_data, list) and query_data:
                query_data = query_data[0]
            
            queries = query_data.get('queries', [])
            
            for query in queries:
                arc_queries.append({
                    'query_text': query['query_text'],
                    'purpose': query.get('purpose', ''),
                    'narrative_arc_id': arc['narrative_arc_id'],
                    'arc_title': arc['title']
                })
                
        except Exception as e:
            logger.warning(f"Failed to generate queries for arc '{arc['title']}': {e}")
            # Fallback query
            arc_queries.append({
                'query_text': f"events related to {arc['title']}",
                'purpose': f"general context for {arc['title']}",
                'narrative_arc_id': arc['narrative_arc_id'],
                'arc_title': arc['title']
            })
    
    logger.info(f"Generated {len(arc_queries)} queries across {len(narrative_arcs)} arcs")
    return arc_queries


def rank_events_per_arc(events_by_arc: Dict[str, List[Any]], episode_plot: str) -> Dict[str, List[Any]]:
    """
    LLM #2: Select top 3 most important events per narrative arc.
    
    Args:
        events_by_arc: Dictionary mapping arc_id to list of events
        episode_plot: Current episode plot for context
        
    Returns:
        Dictionary mapping arc_id to ranked top 3 events
    """
    llm = get_llm(LLMType.INTELLIGENT)
    ranked_events = {}
    
    for arc_id, events in events_by_arc.items():
        if not events:
            continue
            
        # Limit to top 20 events to avoid token limits
        events = events
        
        event_summaries = []
        for i, event in enumerate(events):
            event_summaries.append(f"{i}: [{event.series}{event.season}E{event.episode}] {event.content}")
        
        prompt = f"""Select the 3 most ESSENTIAL events that provide crucial background for understanding the current episode.

**Current Episode Plot:** {episode_plot}...

**Historical Events:**
{chr(10).join(event_summaries)}

Select events that:
1. Directly explain character relationships/conflicts in current episode
2. Show crucial backstory for character motivations
3. Set up major plot developments that matter now

The most important thing is that the events selected should be the most interesting and relevant to the current episode! We are creating a recap to help viewers understand the current episode.

IMPORTANT: Respond with ONLY the event numbers, separated by commas. Example: "0, 5, 12" or "1, 3, 7"

Your answer (just the numbers):"""

        try:
            response = llm.invoke(prompt)
            response_content = response.content if hasattr(response, 'content') else str(response)
            
            logger.debug(f"LLM response for arc {arc_id}: {response_content}...")
            
            # Simple number parsing from comma-separated response
            response_clean = response_content.strip()
            
            # Try to extract numbers directly
            numbers = re.findall(r'\b\d+\b', response_clean)
            selected_indices = [int(x) for x in numbers]  # Take first 3 numbers
            
            # Convert indices to events
            selected_events = []
            for idx in selected_indices:
                if 0 <= idx < len(events):
                    selected_events.append(events[idx])
            
            if selected_events:
                ranked_events[arc_id] = selected_events
                logger.info(f"Arc {arc_id}: selected {len(selected_events)} events")
            else:
                logger.warning(f"Arc {arc_id}: LLM selection failed, using fallback")
                ranked_events[arc_id] = events # Fallback: all events
                logger.warning(f"Arc {arc_id}: LLM selection failed, using fallback")
                ranked_events[arc_id] = events
            
        except Exception as e:
            logger.warning(f"Failed to rank events for arc {arc_id}: {e}")
            # Fallback: take first 3 events
            ranked_events[arc_id] = events
    
    total_selected = sum(len(events) for events in ranked_events.values())
    logger.info(f"Ranked events across {len(ranked_events)} arcs: {total_selected} total events")
    
    return ranked_events


def extract_key_dialogue(events: List[Any], subtitle_data: Dict[str, List[Dict]], ranked_events_by_arc: Dict[str, List[Any]], all_subtitle_data: Dict[str, List[Dict]]) -> Dict[str, List[str]]:
    """
    LLM #3: Extract most meaningful CONSECUTIVE dialogue lines from subtitle spans.
    
    This function implements a round-robin fallback mechanism:
    1. For each selected event, try to extract dialogue from its original timespan
    2. If unsuccessful (no good consecutive dialogue), try the next event in that arc
    3. If successful, record the dialogue with proper attribution to which event was actually used
    
    Args:
        events: List of selected events
        subtitle_data: Dictionary mapping episode to subtitle entries
        ranked_events_by_arc: Dictionary of ranked events by arc for fallback
        all_subtitle_data: All subtitle data for debug purposes
        
    Returns:
        Dictionary mapping event_id to dialogue extraction results
    """
    llm = get_llm(LLMType.INTELLIGENT)
    key_dialogue = {}
    
    for original_event in events:
        logger.info(f"--- Starting dialogue extraction for event {original_event.id} (Arc: {original_event.arc_title}) ---")
        success = False
        
        # Get all events in this arc for fallback
        arc_events = ranked_events_by_arc.get(original_event.narrative_arc_id, [original_event])
        max_attempts = min(3, len(arc_events))  # Try up to 3 events or all available events
        
        last_attempt_inputs = {"subtitles": [], "llm_response": "", "status": "No attempts made"}

        for attempt in range(max_attempts):
            # Select which event to try (round-robin through arc events)
            if attempt < len(arc_events):
                current_event = arc_events[attempt]
            else:
                break  # No more events to try
                
            logger.info(f"Attempt {attempt + 1}/{max_attempts}: Trying event {current_event.id} for original event {original_event.id}")
            
            episode_key = f"{current_event.series}{current_event.season}{current_event.episode}"
            subtitles = subtitle_data.get(episode_key, [])
            
            if not subtitles:
                logger.warning(f"No subtitles found for {episode_key}")
                continue
                
            # Find subtitles in CURRENT event's timespan (not original event!)
            event_subtitles_with_timing = []
            start_seconds = _parse_timestamp_to_seconds(current_event.start_time)
            end_seconds = _parse_timestamp_to_seconds(current_event.end_time)
            
            for i, subtitle in enumerate(subtitles):
                sub_start = _parse_timestamp_to_seconds(subtitle['start'])
                sub_end = _parse_timestamp_to_seconds(subtitle['end'])
                
                # Check if subtitle starts within the CURRENT event timespan
                if sub_start >= start_seconds and sub_start <= end_seconds:
                    event_subtitles_with_timing.append({
                        'text': subtitle['text'],
                        'start': sub_start,
                        'end': sub_end,
                        'start_formatted': subtitle['start'],
                        'end_formatted': subtitle['end'],
                        'original_index': i
                    })
            
            if not event_subtitles_with_timing:
                logger.warning(f"No subtitles found in timespan for event {current_event.id} ({current_event.start_time} - {current_event.end_time})")
                continue
            
            # Sort by start time
            event_subtitles_with_timing.sort(key=lambda x: x['start'])
            
            if len(event_subtitles_with_timing) < 2:
                logger.warning(f"Not enough subtitles for consecutive selection in event {current_event.id} (found {len(event_subtitles_with_timing)})")
                continue

            # Present ALL subtitles to LLM for selection
            subtitle_list = []
            for j, subtitle in enumerate(event_subtitles_with_timing):
                subtitle_list.append(f"{j}: [{subtitle['start_formatted']} - {subtitle['end_formatted']}] {subtitle['text']}")
            
            logger.debug(f"Subtitles presented to LLM for event {current_event.id}:\n{chr(10).join(subtitle_list)}")

            prompt = f"""Select the BEST CONSECUTIVE dialogue sequence for a "Previously On" recap clip.

**Event Context:** {current_event.content}
**Arc:** {current_event.arc_title}
**Event Timespan:** {current_event.start_time} - {current_event.end_time}

**ALL Available Subtitles in Event Timespan:**
{chr(10).join(subtitle_list)}

INSTRUCTIONS:
1. Choose CONSECUTIVE subtitles that form a meaningful dialogue exchange
2. Maximum span: 10 seconds from first to last selected subtitle
3. Minimum: 2 consecutive subtitles
4. Maximum: As many consecutive subtitles as fit within 10 seconds
5. Choose dialogue that best represents this narrative moment

The most important thing is that the subtitles selected should be the most interesting and relevant to the current episode! We are creating a recap to help viewers understand the current episode.

OUTPUT FORMAT:
If good consecutive dialogue exists, respond with ONLY the subtitle numbers (e.g., "0,1,2" or "1,2,3,4")
If no good consecutive dialogue exists, respond just with "SKIP"

Your selection:"""

            last_attempt_inputs = {
                "subtitles": subtitle_list,
                "event_used": current_event.id,
                "event_timespan": f"{current_event.start_time} - {current_event.end_time}",
                "original_event": original_event.id if current_event.id != original_event.id else None
            }

            try:
                response = llm.invoke(prompt)
                response_content = response.content if hasattr(response, 'content') else str(response)
                last_attempt_inputs["llm_response"] = response_content.strip()
                logger.debug(f"LLM raw response for event {current_event.id}: {response_content}")
                response_clean = response_content.strip()
                
                if response_clean.upper() == "SKIP":
                    logger.info(f"LLM decided to skip event {current_event.id}, trying next event")
                    last_attempt_inputs["status"] = "LLM chose to SKIP this event"
                    continue
                
                # Parse selected indices
                try:
                    selected_indices = [int(x.strip()) for x in response_clean.split(',')]
                    logger.debug(f"Successfully parsed indices: {selected_indices}")
                    
                    # Validate indices are consecutive and within range
                    selected_indices.sort()
                    if (len(selected_indices) >= 2 and 
                        all(selected_indices[i] + 1 == selected_indices[i+1] for i in range(len(selected_indices)-1)) and
                        all(0 <= idx < len(event_subtitles_with_timing) for idx in selected_indices)):
                        
                        # Get the selected subtitles (using the indices from our ordered list)
                        selected_subs = [event_subtitles_with_timing[idx] for idx in selected_indices]

                        # Check if selection fits within 10 seconds
                        first_sub = selected_subs[0]
                        last_sub = selected_subs[-1]
                        duration = last_sub['end'] - first_sub['start']
                        
                        if duration <= 10.0:
                            # Extract selected subtitles
                            selected_lines = [sub['text'] for sub in selected_subs]
                            
                            if selected_lines:
                                key_dialogue[original_event.id] = {
                                    'lines': selected_lines,
                                    'start_time': first_sub['start_formatted'],
                                    'end_time': last_sub['end_formatted'],
                                    'source_event_id': current_event.id,
                                    'debug': {
                                        "event_used": current_event.id,
                                        "event_timespan": f"{current_event.start_time} - {current_event.end_time}",
                                        "subtitles_sent_to_llm": subtitle_list,
                                        "llm_response": response_clean,
                                        "selected_indices": selected_indices,
                                        "duration_seconds": round(duration, 1),
                                        "status": f"SUCCESS - Used event {current_event.id} for original event {original_event.id}"
                                    }
                                }
                                logger.info(f"‚úÖ Event {original_event.id}: extracted {len(selected_lines)} consecutive dialogue lines from event {current_event.id} in episode {episode_key} at {first_sub['start_formatted']} ({duration:.1f}s)")
                                success = True
                                break
                        else:
                            last_attempt_inputs["status"] = f"Selected dialogue span too long ({duration:.1f}s > 10s)"
                            logger.warning(f"Selected dialogue span for event {current_event.id} too long ({duration:.1f}s > 10s), trying next event")
                    else:
                        last_attempt_inputs["status"] = "Invalid selection (not consecutive or out of range)"
                        logger.warning(f"Invalid selection (not consecutive or out of range), trying next event")
                        
                except (ValueError, IndexError) as e:
                    last_attempt_inputs["status"] = f"Could not parse LLM selection: {e}"
                    logger.warning(f"Could not parse LLM selection '{response_clean}': {e}")
                
            except Exception as e:
                last_attempt_inputs["status"] = f"LLM call failed: {e}"
                logger.warning(f"Failed to extract dialogue for event {current_event.id}: {e}")
        
        # If no success after trying all events in the arc
        if not success:
            # Get the original event's subtitle content for debug
            episode_key = f"{original_event.series}{original_event.season}{original_event.episode}"
            subtitles_in_original_event = []
            if episode_key in all_subtitle_data:
                start_seconds = _parse_timestamp_to_seconds(original_event.start_time)
                end_seconds = _parse_timestamp_to_seconds(original_event.end_time)
                for sub in all_subtitle_data[episode_key]:
                    sub_start = _parse_timestamp_to_seconds(sub['start'])
                    if sub_start >= start_seconds and sub_start <= end_seconds:
                        subtitles_in_original_event.append(sub['text'])
            
            key_dialogue[original_event.id] = {
                "lines": [],
                "start_time": "",
                "end_time": "",
                "debug": {
                    **last_attempt_inputs,
                    "subtitles_in_original_event": subtitles_in_original_event,
                    "original_event_timespan": f"{original_event.start_time} - {original_event.end_time}",
                    "status": f"FAILED after {max_attempts} attempts"
                }
            }
            logger.warning(f"‚ùå Event {original_event.id}: failed to extract dialogue after trying {max_attempts} events in arc")

    logger.info(f"Extracted dialogue for {len([k for k, v in key_dialogue.items() if v['lines']])} out of {len(events)} events")
    return key_dialogue
    


def _parse_timestamp_to_seconds(timestamp: str) -> float:
    """Convert HH:MM:SS,mmm timestamp to seconds."""
    try:
        if ',' in timestamp:
            time_part, ms_part = timestamp.split(',')
            h, m, s = map(int, time_part.split(':'))
            ms = int(ms_part)
            return h * 3600 + m * 60 + s + ms / 1000.0
        else:
            h, m, s = map(int, timestamp.split(':'))
            return h * 3600 + m * 60 + s
    except:
        return 0.0
"""
Simple data models for recap generation.
"""

from dataclasses import dataclass
from typing import List, Optional, Dict, Any


@dataclass
class Event:
    """Represents a selected narrative event."""
    id: str
    content: str
    series: str
    season: str
    episode: str
    start_time: str  # HH:MM:SS,mmm format
    end_time: str    # HH:MM:SS,mmm format
    narrative_arc_id: str
    arc_title: str
    relevance_score: float = 0.0

    def to_dict(self) -> Dict[str, Any]:
        """Convert event to dictionary for JSON serialization."""
        return {
            "id": self.id,
            "content": self.content,
            "series": self.series,
            "season": self.season,
            "episode": self.episode,
            "start_time": self.start_time,
            "end_time": self.end_time,
            "narrative_arc_id": self.narrative_arc_id,
            "arc_title": self.arc_title,
            "relevance_score": self.relevance_score
        }


@dataclass
class VideoClip:
    """Represents an extracted video clip."""
    event_id: str
    file_path: str
    start_seconds: float
    end_seconds: float
    duration: float
    subtitle_lines: List[str]
    arc_title: str


@dataclass
class RecapResult:
    """Final recap generation result."""
    video_path: str
    events: List[Event]
    clips: List[VideoClip]
    total_duration: float
    success: bool
    error_message: Optional[str] = None
# Recap Generation Workflow

This document provides a detailed breakdown of the "Previously On" recap generation pipeline. The system is designed to automatically create recap videos by identifying relevant past events, extracting key dialogue, and assembling video clips.

The entire workflow is orchestrated by the `RecapGenerator` class (`recap_generator.py`).

## High-Level Overview

The pipeline can be summarized in the following steps:

1.  **Initialization & Validation**: Set up the generator and validate that all required source files for the target episode are present.
2.  **Input Loading**: Load the necessary data for the target episode, including plot, narrative arcs, and subtitle files.
3.  **LLM #1: Query Generation**: For each narrative arc in the current episode, generate search queries to find relevant historical events.
4.  **Vector DB Search**: Use the generated queries to search a vector database of past events.
5.  **LLM #2: Event Ranking**: For each narrative arc, use an LLM to rank the retrieved events by importance and relevance to the current episode.
6.  **Final Event Selection**: Select a final list of events for the recap, ensuring diversity across narrative arcs using a round-robin strategy.
7.  **LLM #3: Dialogue Pruning**: For each selected event, use an LLM to identify the most impactful and consecutive lines of dialogue from the subtitles. This step includes a fallback mechanism to ensure quality.
8.  **Video Clip Extraction**: Use FFmpeg to extract the video segments corresponding to the selected dialogue.
9.  **Final Assembly**: Concatenate the individual clips into a single recap video file.
10. **Metadata Export**: Save a detailed JSON file that specifies which events and dialogue were used in the final recap for transparency and debugging.

---

## Detailed Step-by-Step Workflow

### 1. Initialization and Prerequisites (`recap_generator.py`)

-   The `RecapGenerator` is initialized with a `base_dir` pointing to the root of the data directory (e.g., `data/`).
-   Before running the main pipeline, the `validate_prerequisites` method is called.
-   It uses `PathHandler` to check for the existence of critical input files for the target episode:
    -   The episode's plot summary (`plot_with_possible_speakers.txt`).
    -   The active narrative arcs for the episode (`present_running_plotlines.json`).
    -   The source video file for the episode (e.g., `E09.mp4`).

### 2. Loading Episode Inputs (`utils.py`)

-   The `load_episode_inputs` function gathers all necessary data:
    -   **Episode Plot**: The main plot of the current episode.
    -   **Season Summary**: A summary of the entire season for broader context.
    -   **Narrative Arcs**: The list of running plotlines active in the current episode.
    -   **Subtitle Data**: It loads all subtitle files for the entire season to have them ready for the dialogue extraction phase. It prioritizes the speaker-identified subtitle files (`possible_speakers.srt`).

### 3. LLM #1: Generate Arc Queries (`llm_services.py`)

-   The `generate_arc_queries` function is the first LLM call in the pipeline.
-   For each narrative arc, it creates a prompt that includes the season summary, the current episode's plot, and the arc's title and description.
-   It asks the LLM to generate 2-3 specific, natural language search queries to find historical events that provide context for that arc.

### 4. Vector Database Search (`utils.py`)

-   The `search_vector_database` function takes the queries from the previous step.
-   It connects to the `VectorStoreService` (ChromaDB) to find similar events.
-   **Crucially, it builds an exclusion list to prevent searching the current or any future episodes**, ensuring the recap only contains past events.
-   The search results are grouped by their respective narrative arcs.

### 5. LLM #2: Rank Events Per Arc (`llm_services.py`)

-   The `rank_events_per_arc` function performs the second LLM call.
-   For each narrative arc, it presents the LLM with the list of events found in the vector database.
-   It prompts the LLM to select the **top 3 most essential events** that provide crucial background for the current episode.

### 6. Final Event Selection (`utils.py`)

-   The `select_events_round_robin` function creates the final list of events for the recap (defaulting to a maximum of 8).
-   It uses a **round-robin** selection strategy:
    1.  It first picks the top-ranked event from *every* narrative arc to guarantee coverage.
    2.  It then performs additional rounds, picking the next-best event from each arc until the maximum number of events is reached.

### 7. LLM #3: Extract Key Dialogue (`llm_services.py`)

-   The `extract_key_dialogue` function is the third and most complex LLM call.
-   For each event in the final selection, it retrieves the corresponding subtitles.
-   It prompts the LLM to select the **best consecutive sequence of dialogue** that is meaningful and fits within a 10-second window.
-   **Fallback Mechanism**: If the LLM determines there is no good dialogue in the originally selected event (or if the process fails), it has a fallback. It will attempt the same dialogue extraction process on the *other ranked events* from the same narrative arc. This significantly increases the quality and success rate of the generated clips.

### 8. Video Clip Extraction (`video_processor.py`)

-   The `extract_video_clips` function orchestrates the use of FFmpeg.
-   For each event, it uses the precise start and end times of the **dialogue selected by LLM #3** (not the original event's full timespan).
-   It calls a helper function (`_extract_clip_ffmpeg`) to run an FFmpeg command that cuts the clip from the source video file.
-   The extracted clips are saved in a dedicated directory managed by `PathHandler`.

### 9. Final Recap Assembly (`video_processor.py`)

-   The `assemble_final_recap` function completes the video generation.
-   It creates a text file listing all the individual clip paths.
-   It then uses FFmpeg's `concat` demuxer to stitch all the clips together into a single, final video file (e.g., `recap_S01_E09.mp4`).

### 10. JSON Export (`recap_generator.py`)

-   As a final step, the `generate_recap` method creates a detailed JSON file (`recap_clips.json`).
-   This file serves as a "spec" for the generated recap and contains:
    -   The list of all ranked events considered for each arc.
    -   The final list of selected events.
    -   The exact dialogue lines that were chosen for each clip.
    -   Debug information from the dialogue extraction step, including which event was used (original or fallback) and the LLM's raw response.
"""
Main recap generator class - simplified and streamlined.

This module provides the core RecapGenerator class that orchestrates
the entire recap generation process following the specified LLM workflow.
"""

import logging
import os
import sys
import json
from typing import Optional

# Add the src directory to Python path to enable proper imports
current_dir = os.path.dirname(__file__)
src_dir = os.path.dirname(os.path.dirname(current_dir))
if src_dir not in sys.path:
    sys.path.insert(0, src_dir)

from path_handler import PathHandler
from .models import RecapResult
from .utils import load_episode_inputs, select_events_round_robin, search_vector_database
from .llm_services import generate_arc_queries, rank_events_per_arc, extract_key_dialogue
from .video_processor import extract_video_clips, assemble_final_recap

logger = logging.getLogger(__name__)


class RecapGenerator:
    """
    Simplified recap generator that follows the core LLM workflow:
    
    1. LLM #1: Query generation per narrative arc
    2. Vector DB search for historical events
    3. LLM #2: Event ranking per arc (loop)
    4. Round-robin final event selection
    5. LLM #3: Subtitle pruning (round robin)
    6. FFmpeg video clip extraction
    7. Final recap assembly
    """
    
    def __init__(self, base_dir: str = "data"):
        self.base_dir = base_dir
        
    def generate_recap(self, series: str, season: str, episode: str) -> RecapResult:
        """
        Generate a "Previously On" recap for the specified episode.
        
        Args:
            series: Series identifier (e.g., "GreysAnatomy")
            season: Season identifier (e.g., "S01")
            episode: Episode identifier (e.g., "E09")
            
        Returns:
            RecapResult with video path and metadata
        """
        try:
            logger.info(f"üé¨ Starting recap generation for {series} {season} {episode}")
            
            # Step 1: Load episode inputs
            logger.info("üìñ Loading episode inputs...")
            inputs = load_episode_inputs(series, season, episode, self.base_dir)
            
            if not inputs['narrative_arcs']:
                return RecapResult(
                    video_path="", events=[], clips=[], 
                    total_duration=0.0, success=False,
                    error_message="No narrative arcs found"
                )
            
            # Step 2: LLM #1 - Generate queries per narrative arc
            logger.info("üéØ Generating vector database queries...")
            arc_queries = generate_arc_queries(
                inputs['season_summary'],
                inputs['episode_plot'], 
                inputs['narrative_arcs']
            )
            
            if not arc_queries:
                return RecapResult(
                    video_path="", events=[], clips=[],
                    total_duration=0.0, success=False,
                    error_message="No queries generated"
                )
            
            # Step 3: Search vector database
            logger.info("üîç Searching vector database...")
            events_by_arc = search_vector_database(arc_queries, series, season, episode)
            
            if not events_by_arc:
                return RecapResult(
                    video_path="", events=[], clips=[],
                    total_duration=0.0, success=False,
                    error_message="No events found in vector database"
                )
            
            # Step 4: LLM #2 - Rank events per arc (loop)
            logger.info("üìä Ranking events per narrative arc...")
            ranked_events = rank_events_per_arc(events_by_arc, inputs['episode_plot'])
            logger.info("üìä Event ranking complete.")
            
            # Step 5: Round-robin final event selection
            logger.info("üéØ Selecting final events (round-robin)...")
            selected_events = select_events_round_robin(ranked_events, max_events=8)
            
            if not selected_events:
                return RecapResult(
                    video_path="", events=[], clips=[],
                    total_duration=0.0, success=False,
                    error_message="No events selected"
                )
            
            # Step 6: LLM #3 - Extract key dialogue with consecutive subtitles and fallback
            logger.info("üìù Extracting key dialogue...")
            key_dialogue = extract_key_dialogue(selected_events, inputs['subtitle_data'], ranked_events, inputs['subtitle_data'])

            # Save a JSON spec of selected events and pruned subtitles for transparency
            try:
                path_handler = PathHandler(series, season, episode, self.base_dir)
                recap_dir = path_handler.get_recap_files_dir()
                os.makedirs(recap_dir, exist_ok=True)
                recap_spec_path = path_handler.get_recap_clips_json_path()

                # Build export structure grouped by arc
                export = {
                    "series": series,
                    "season": season,
                    "episode": episode,
                    "ranked_events_by_arc": {arc_id: [ev.to_dict() for ev in ev_list] for arc_id, ev_list in ranked_events.items()},
                    "selected_events": []
                }
                for ev in selected_events:
                    export["selected_events"].append({
                        "event_id": ev.id,
                        "arc_title": ev.arc_title,
                        "narrative_arc_id": getattr(ev, 'narrative_arc_id', ''),
                        "source_episode": f"{ev.series}{ev.season}{ev.episode}",
                        "start_time": ev.start_time,
                        "end_time": ev.end_time,
                        "content": ev.content,
                        "selected_subtitles": key_dialogue.get(ev.id, {}).get('lines', []),
                        "debug_info": key_dialogue.get(ev.id, {}).get('debug', {})
                    })

                with open(recap_spec_path, 'w', encoding='utf-8') as f:
                    json.dump(export, f, ensure_ascii=False, indent=2)
                logger.info(f"üìù Saved recap clips JSON: {recap_spec_path}")
            except Exception as e:
                logger.warning(f"Couldn't write recap clips JSON: {e}")
            
            # Step 7: Extract video clips
            logger.info("üé¨ Extracting video clips...")
            video_clips = extract_video_clips(selected_events, key_dialogue, self.base_dir)
            
            if not video_clips:
                return RecapResult(
                    video_path="", events=selected_events, clips=[],
                    total_duration=0.0, success=False,
                    error_message="No video clips extracted"
                )
            
            # Step 8: Assemble final recap
            logger.info("üéûÔ∏è Assembling final recap...")
            path_handler = PathHandler(series, season, episode, self.base_dir)
            output_dir = path_handler.get_recap_files_dir()  # This will be used by assemble_final_recap
            final_video_path = assemble_final_recap(video_clips, output_dir, series, season, episode)
            
            # Calculate total duration
            total_duration = sum(clip.duration for clip in video_clips)
            
            logger.info(f"‚úÖ Recap generation completed successfully!")
            logger.info(f"üìÅ Final video: {final_video_path}")
            logger.info(f"‚è±Ô∏è Total duration: {total_duration:.1f}s")
            logger.info(f"üé¨ Clips: {len(video_clips)} from {len(selected_events)} events")
            
            return RecapResult(
                video_path=final_video_path,
                events=selected_events,
                clips=video_clips,
                total_duration=total_duration,
                success=True
            )
            
        except Exception as e:
            logger.error(f"‚ùå Recap generation failed: {e}")
            return RecapResult(
                video_path="", events=[], clips=[],
                total_duration=0.0, success=False,
                error_message=str(e)
            )
    
    def validate_prerequisites(self, series: str, season: str, episode: str) -> dict:
        """
        Validate that all required files exist for recap generation.
        
        Returns:
            Dictionary with validation results
        """
        path_handler = PathHandler(series, season, episode, self.base_dir)
        
        results = {
            'ready': True,
            'missing_files': [],
            'warnings': []
        }
        
        # Required files using PathHandler
        required_files = [
            ("plot_possible_speakers", path_handler.get_plot_possible_speakers_path()),
            ("present_running_plotlines", path_handler.get_present_running_plotlines_path()),
            ("video_file", path_handler.get_video_file_path())
        ]
        
        for file_type, file_path in required_files:
            if not os.path.exists(file_path):
                results['ready'] = False
                results['missing_files'].append(f"{file_type}: {file_path}")
        
        # Optional but recommended files
        season_summary = path_handler.get_season_summary_path()
        if not os.path.exists(season_summary):
            results['warnings'].append(f"Season summary not found: {season_summary}")
        
        return results
"""
Utility functions for recap generation.
"""

import os
import json
import sqlite3
import logging
import sys
from typing import List, Dict, Any, Optional

# Add the src directory to Python path to enable proper imports
current_dir = os.path.dirname(__file__)
src_dir = os.path.dirname(os.path.dirname(current_dir))
if src_dir not in sys.path:
    sys.path.insert(0, src_dir)

# Add the backend/src directory to sys.path for absolute imports
backend_src = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
if backend_src not in sys.path:
    sys.path.insert(0, backend_src)

from narrative_storage_management.vector_store_service import VectorStoreService
from path_handler import PathHandler

logger = logging.getLogger(__name__)


def load_episode_inputs(series: str, season: str, episode: str, base_dir: str = "data") -> Dict[str, Any]:
    """
    Load all required input files for recap generation.
    
    Args:
        series: Series identifier
        season: Season identifier 
        episode: Episode identifier
        base_dir: Base data directory
        
    Returns:
        Dictionary containing episode inputs
    """
    path_handler = PathHandler(series, season, episode, base_dir)
    
    inputs = {
        'series': series,
        'season': season, 
        'episode': episode
    }
    
    # Load episode plot
    plot_file = path_handler.get_plot_possible_speakers_path()
    if os.path.exists(plot_file):
        with open(plot_file, 'r', encoding='utf-8') as f:
            inputs['episode_plot'] = f.read().strip()
    else:
        logger.warning(f"Plot file not found: {plot_file}")
        inputs['episode_plot'] = ""
    
    # Load season summary
    summary_file = path_handler.get_season_summary_path()
    if os.path.exists(summary_file):
        with open(summary_file, 'r', encoding='utf-8') as f:
            inputs['season_summary'] = f.read().strip()
    else:
        logger.warning(f"Season summary not found: {summary_file}")
        inputs['season_summary'] = ""
    
    # Load running plotlines
    plotlines_file = path_handler.get_present_running_plotlines_path()
    if os.path.exists(plotlines_file):
        with open(plotlines_file, 'r', encoding='utf-8') as f:
            plotlines_data = json.load(f)
            
        # Handle different formats
        if isinstance(plotlines_data, list):
            narrative_arcs = plotlines_data
        else:
            narrative_arcs = plotlines_data.get('running_plotlines', [])
        
        # Enrich with narrative arc IDs
        for arc in narrative_arcs:
            arc_id = get_narrative_arc_id(arc.get('title', ''))
            arc['narrative_arc_id'] = arc_id
            
        inputs['narrative_arcs'] = narrative_arcs
    else:
        logger.warning(f"Plotlines file not found: {plotlines_file}")
        inputs['narrative_arcs'] = []
    
    # Load subtitle data
    inputs['subtitle_data'] = load_subtitle_data(path_handler)
    
    logger.info(f"Loaded inputs for {series}{season}{episode}: "
               f"{len(inputs['narrative_arcs'])} arcs, "
               f"{len(inputs.get('subtitle_data', {}))} subtitle files")
    
    return inputs


def get_narrative_arc_id(title: str) -> Optional[str]:
    """Get narrative arc ID from database by title."""
    try:
        db_path = "narrative_storage/narrative.db"
        if not os.path.exists(db_path):
            return None
        
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        cursor.execute("SELECT id FROM narrativearc WHERE title = ?", (title,))
        result = cursor.fetchone()
        conn.close()
        
        return result[0] if result else None
        
    except Exception as e:
        logger.warning(f"Failed to lookup arc ID for '{title}': {e}")
        return None


def load_subtitle_data(path_handler: PathHandler) -> Dict[str, List[Dict]]:
    """
    Load subtitle data for episodes that might be referenced.
    
    Uses PathHandler to get the correct subtitle files (prioritizing possible_speakers.srt).
    """
    subtitle_data = {}
    
    # Get season directory from path handler
    series = path_handler.get_series()
    season = path_handler.get_season()
    base_dir = path_handler.base_dir
    season_dir = f"{base_dir}/{series}/{season}"
    
    if not os.path.exists(season_dir):
        return subtitle_data
    
    # Load subtitles for all episodes in season
    for ep_dir in os.listdir(season_dir):
        ep_path = os.path.join(season_dir, ep_dir)
        if not os.path.isdir(ep_path):
            continue
        
        try:
            # Extract episode info (e.g., "E01" from "E01")
            if len(ep_dir) >= 3 and ep_dir.startswith('E'):
                episode = ep_dir  # ep_dir is already "E01", "E02", etc.
            else:
                continue
            
            # Create PathHandler for this specific episode
            episode_path_handler = PathHandler(series, season, episode, base_dir)
            
            # Create proper episode key format (e.g., "GAS01E01")
            episode_key = f"{series}{season}{episode}"  # Build the full key
            
            # Try to load possible_speakers.srt first (preferred)
            possible_speakers_path = episode_path_handler.get_possible_speakers_srt_path()
            if os.path.exists(possible_speakers_path):
                subtitle_entries = parse_srt_file(possible_speakers_path)
                subtitle_data[episode_key] = subtitle_entries
                logger.debug(f"Loaded possible_speakers subtitles for {episode_key}: {len(subtitle_entries)} entries")
                continue
            
            # Fallback to regular .srt file
            regular_srt_path = episode_path_handler.get_srt_file_path()
            if os.path.exists(regular_srt_path):
                subtitle_entries = parse_srt_file(regular_srt_path)
                subtitle_data[episode_key] = subtitle_entries
                logger.debug(f"Loaded regular subtitles for {episode_key}: {len(subtitle_entries)} entries")
                continue
                
            logger.debug(f"No subtitle files found for episode {episode_key}")
            
        except Exception as e:
            logger.warning(f"Failed to load subtitles for {ep_dir}: {e}")
            continue
    
    logger.info(f"Loaded subtitles for {len(subtitle_data)} episodes")
    return subtitle_data


def parse_srt_file(srt_path: str) -> List[Dict]:
    """
    Parse an SRT subtitle file.
    
    Returns:
        List of subtitle entries with start/end times and text
    """
    entries = []
    
    try:
        with open(srt_path, 'r', encoding='utf-8') as f:
            content = f.read().strip()
        
        blocks = content.split('\n\n')  # Fixed: use actual newlines, not literal \\n
        
        for block in blocks:
            lines = block.split('\n')  # Fixed: use actual newlines, not literal \\n
            if len(lines) >= 3:
                # Skip sequence number (first line)
                timestamp_line = lines[1]
                text_lines = lines[2:]
                
                # Parse timestamp: 00:00:20,000 --> 00:00:24,400
                if ' --> ' in timestamp_line:
                    start_time, end_time = timestamp_line.split(' --> ')
                    
                    entries.append({
                        'start': start_time.strip(),
                        'end': end_time.strip(),
                        'text': ' '.join(text_lines).strip()
                    })
    
    except Exception as e:
        logger.warning(f"Failed to parse SRT file {srt_path}: {e}")
    
    return entries


def select_events_round_robin(ranked_events_by_arc: Dict[str, List[Any]], max_events: int = 8) -> List[Any]:
    """
    Select final events using round-robin approach to ensure arc coverage.
    
    Args:
        ranked_events_by_arc: Dict mapping arc_id to ranked events
        max_events: Maximum number of events to select
        
    Returns:
        List of selected events
    """
    selected_events = []
    arc_ids = list(ranked_events_by_arc.keys())
    
    if not arc_ids:
        return selected_events
    
    # Round 1: Select one event from each arc (guaranteed coverage)
    for arc_id in arc_ids:
        events = ranked_events_by_arc[arc_id]
        if events:
            selected_events.append(events[0])
    
    # Round 2-3: Add additional events while under limit
    round_num = 1
    while len(selected_events) < max_events and round_num < 3:
        added_this_round = 0
        
        for arc_id in arc_ids:
            if len(selected_events) >= max_events:
                break
                
            events = ranked_events_by_arc[arc_id]
            if len(events) > round_num:
                selected_events.append(events[round_num])
                added_this_round += 1
        
        if added_this_round == 0:
            break
            
        round_num += 1
    
    logger.info(f"Round-robin selection: {len(selected_events)} events from {len(arc_ids)} arcs")
    return selected_events


def search_vector_database(queries: List[Dict[str, Any]], current_series: str = None, current_season: str = None, current_episode: str = None) -> Dict[str, List[Any]]:
    """
    Search vector database for events matching the queries.
    
    Args:
        queries: List of query dictionaries
        current_series: Current series for exclusion list
        current_season: Current season for exclusion list  
        current_episode: Current episode for exclusion list
        
    Returns:
        Dictionary mapping arc_id to list of matching events
    """
    try:
        from .models import Event
        
        # Initialize vector service using the same approach as working system
        vector_service = VectorStoreService()
        events_by_arc = {}
        
        # Build exclusion list (exclude current and future episodes)
        if current_season and current_episode:
            exclude_episodes = build_exclusion_list(current_season, current_episode)
        elif queries:
            exclude_episodes = build_exclusion_list('S01', 'E09')  # Default for testing
        else:
            exclude_episodes = []
        
        logger.info(f"üîç Searching vector database for {len(queries)} queries")
        logger.info(f"üö´ Excluding {len(exclude_episodes)} future episodes from search")
        
        for query in queries:
            try:
                logger.debug(f"Executing query for arc {query['narrative_arc_id'][:8]}: '{query['query_text']}'")
                # Search vector database using SAME method as original
                results = vector_service.find_similar_events(
                    query['query_text'],
                    n_results=10,  # Same as original
                    series=current_series or 'GA',
                    narrative_arc_ids=[query['narrative_arc_id']] if query.get('narrative_arc_id') else None,
                    exclude_episodes=exclude_episodes
                )
                
                # Convert results to Event objects
                arc_id = query['narrative_arc_id']
                if arc_id not in events_by_arc:
                    events_by_arc[arc_id] = []
                
                for result in results:
                    # Extract metadata from vector search result
                    metadata = result.get('metadata', {})
                    
                    # Create Event object with proper data from vector search
                    event = Event(
                        id=metadata.get('id', ''),
                        content=result.get('page_content', ''),
                        series=metadata.get('series', ''),
                        season=metadata.get('season', ''),
                        episode=metadata.get('episode', ''),
                        start_time=metadata.get('start_timestamp', '00:00:00,000'),
                        end_time=metadata.get('end_timestamp', '00:00:00,000'),
                        narrative_arc_id=arc_id,
                        arc_title=query.get('arc_title', ''),
                        relevance_score=1.0 - result.get('cosine_distance', 1.0)  # Convert distance to similarity
                    )
                    events_by_arc[arc_id].append(event)
                    
                logger.debug(f"üéØ Query for arc {arc_id[:8]}... found {len(results)} events")
                
            except Exception as e:
                logger.warning(f"Vector search failed for query {query.get('query_text', '')[:30]}...: {e}")
                continue
        
        total_events = sum(len(events) for events in events_by_arc.values())
        logger.info(f"‚úÖ Vector search completed: {total_events} events across {len(events_by_arc)} arcs")
        
        return events_by_arc
        
    except Exception as e:
        logger.error(f"‚ùå Vector database search failed: {e}")
        import traceback
        traceback.print_exc()
        raise e  # Don't return empty dict, raise the error so we know what's wrong


def build_exclusion_list(current_season: str, current_episode: str) -> List[tuple]:
    """Build list of episodes to exclude (current + future)."""
    exclude_episodes = []
    
    try:
        season_num = int(current_season[1:])  # S01 -> 1
        episode_num = int(current_episode[1:])  # E09 -> 9
    except:
        return [(current_season, current_episode)]
    
    # Exclude current episode
    exclude_episodes.append((current_season, current_episode))
    
    # Exclude future episodes in current season
    for ep in range(episode_num + 1, 25):
        exclude_episodes.append((current_season, f"E{ep:02d}"))
    
    # Exclude future seasons  
    for season in range(season_num + 1, 21):
        for ep in range(1, 25):
            exclude_episodes.append((f"S{season:02d}", f"E{ep:02d}"))
    
    return exclude_episodes
"""
Video processing utilities for recap generation.

This module handles FFmpeg operations for video clip extraction and assembly.
"""

import os
import subprocess
import logging
import sys
from typing import List, Dict, Any

# Add the src directory to Python path to enable proper imports
current_dir = os.path.dirname(__file__)
src_dir = os.path.dirname(os.path.dirname(current_dir))
if src_dir not in sys.path:
    sys.path.insert(0, src_dir)

from path_handler import PathHandler
from .models import Event, VideoClip

logger = logging.getLogger(__name__)


def extract_video_clips(events: List[Event], key_dialogue: Dict[str, List[str]], base_data_dir: str = "data") -> List[VideoClip]:
    """
    Extract video clips for selected events using FFmpeg.
    
    Args:
        events: List of selected events
        key_dialogue: Dictionary mapping event_id to dialogue lines
        base_data_dir: Base directory for episode data
        
    Returns:
        List of extracted video clips
    """
    clips = []
    
    for event in events:
        try:
            # Create PathHandler for this event's episode
            path_handler = PathHandler(event.series, event.season, event.episode, base_data_dir)
            
            # Find source video file
            video_path = path_handler.get_video_file_path()
            if not os.path.exists(video_path):
                logger.warning(f"No video file found for {event.series}{event.season}{event.episode}")
                continue
            
            # Get dialogue info, including new start/end times
            dialogue_info = key_dialogue.get(event.id)
            if not dialogue_info or not dialogue_info.get('lines'):
                logger.warning(f"No dialogue found for event {event.id}, skipping clip")
                continue

            # Calculate clip timestamps from dialogue
            start_seconds = _parse_timestamp_to_seconds(dialogue_info['start_time'])
            end_seconds = _parse_timestamp_to_seconds(dialogue_info['end_time'])
            duration = end_seconds - start_seconds
            
            # Create output directory using PathHandler
            output_dir = path_handler.get_recap_clip_dir()
            os.makedirs(output_dir, exist_ok=True)

            # Generate output filename using PathHandler helper (stable naming)
            clip_id = event.id  # Use full event ID
            clip_path = path_handler.get_individual_clip_path(clip_id)
            
            # Extract clip using FFmpeg
            success = _extract_clip_ffmpeg(video_path, clip_path, start_seconds, duration)
            
            if success:
                clips.append(VideoClip(
                    event_id=event.id,
                    file_path=clip_path,
                    start_seconds=start_seconds,
                    end_seconds=end_seconds,
                    duration=duration,
                    subtitle_lines=dialogue_info['lines'],
                    arc_title=event.arc_title
                ))
                logger.info(f"Extracted clip: {event.arc_title} ({duration:.1f}s)")
            
        except Exception as e:
            logger.warning(f"Failed to extract clip for event {event.id}: {e}")
    
    logger.info(f"Successfully extracted {len(clips)} video clips")
    return clips


def assemble_final_recap(clips: List[VideoClip], output_dir: str, series: str, season: str, episode: str) -> str:
    """
    Assemble individual clips into final recap video.
    
    Args:
        clips: List of video clips to assemble
        output_dir: Directory to save final recap (will use PathHandler)
        series: Series identifier
        season: Season identifier
        episode: Episode identifier
        
    Returns:
        Path to final recap video
    """
    if not clips:
        raise ValueError("No clips provided for assembly")
    
    # Use PathHandler for proper path management
    path_handler = PathHandler(series, season, episode, "data")  # Use data as base_dir
    
    # Create output directory using PathHandler
    recap_dir = path_handler.get_recap_files_dir()
    os.makedirs(recap_dir, exist_ok=True)
    
    # Final recap filename using PathHandler
    recap_path = path_handler.get_final_recap_video_path()
    
    try:
        # Create file list for FFmpeg concatenation
        filelist_path = os.path.join(recap_dir, "clips_list.txt")
        with open(filelist_path, 'w') as f:
            for clip in clips:
                f.write(f"file '{os.path.abspath(clip.file_path)}'\n")
        
        # Concatenate clips using FFmpeg
        cmd = [
            'ffmpeg', '-y',
            '-f', 'concat',
            '-safe', '0',
            '-i', filelist_path,
            '-c', 'copy',
            recap_path
        ]
        
        result = subprocess.run(cmd, capture_output=True, text=True)
        
        if result.returncode == 0:
            logger.info(f"Successfully assembled recap: {recap_path}")
            
            # Clean up temporary files
            # os.remove(filelist_path)
            
            return recap_path
        else:
            raise RuntimeError(f"FFmpeg concatenation failed: {result.stderr}")
            
    except Exception as e:
        logger.error(f"Failed to assemble final recap: {e}")
        raise


def _find_episode_video(series: str, season: str, episode: str, base_dir: str) -> str:
    """Find the video file for a given episode using PathHandler."""
    path_handler = PathHandler(series, season, episode, base_dir)
    video_path = path_handler.get_video_file_path()
    
    if os.path.exists(video_path):
        return video_path
    
    # Fallback: look for other video files in the episode directory
    episode_dir = f"{base_dir}/{series}/{season}/{episode}"
    video_extensions = ['.mp4', '.mkv', '.avi', '.mov']
    
    if os.path.exists(episode_dir):
        for file in os.listdir(episode_dir):
            for ext in video_extensions:
                if file.endswith(ext) and not file.startswith('recap_'):
                    return os.path.join(episode_dir, file)
    
    return None


def _extract_clip_ffmpeg(input_path: str, output_path: str, start_seconds: float, duration: float) -> bool:
    """Extract a video clip using FFmpeg."""
    try:
        cmd = [
            'ffmpeg', '-y',
            '-i', input_path,
            '-ss', str(start_seconds),
            '-t', str(duration),
            '-c:v', 'libx264',
            '-c:a', 'aac',
            '-avoid_negative_ts', 'make_zero',
            output_path
        ]
        
        result = subprocess.run(cmd, capture_output=True, text=True)
        
        if result.returncode == 0:
            return True
        else:
            logger.error(f"FFmpeg extraction failed: {result.stderr}")
            return False
            
    except Exception as e:
        logger.error(f"FFmpeg command failed: {e}")
        return False


def _parse_timestamp_to_seconds(timestamp: str) -> float:
    """Convert HH:MM:SS,mmm timestamp to seconds."""
    try:
        if ',' in timestamp:
            time_part, ms_part = timestamp.split(',')
            h, m, s = map(int, time_part.split(':'))
            ms = int(ms_part)
            return h * 3600 + m * 60 + s + ms / 1000.0
        else:
            h, m, s = map(int, timestamp.split(':'))
            return h * 3600 + m * 60 + s
    except:
        return 0.0
